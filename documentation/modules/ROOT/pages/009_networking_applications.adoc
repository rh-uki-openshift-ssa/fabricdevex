= Understanding Application Networking
:!sectids:

image::009-image001.png[The Services in a Project]

== *Understanding Application Networking*

====
*What will you learn?*

This Module will go into depth around the mechanisms for Networking of Applications within OpenShift. This will cover the Services, which are bound to the Project, Routes, which provide the external traffic connectivity and inter-Application networking. It will also explain Network Policies which allow for fine-grain control of traffic to a Service.

The Module will include some practical examples of Networking using three distinct Services in the Project

After completing this Module you will understand the nature of the abstracted SDN and the Service endpoints, how to setup Routes and what capabilities inter-Application Networking provides for Developers
====

=== *Pre-Requisites*

In order to undertake the Module you need to be logged onto the OpenShift cluster and have access to a Project. If you do not have rights to create a project, ask the Cluster administrators to provide one for you.

*Ensure the two textboxes at the top of this HTML Guide to the Module contain the Cluster address and the Project you will be using.* 

[NOTE]
====
A *Project* is an OpenShift object that contains all of your Applications and other components in a compartmented way. +
As the owner/user of a Project you have 'Admin' rights to it, meaning you can create any of the OpenShift objects that you have the rights to create within it.  
====

[WARNING]
====
*Please Read* - if you are using a *shared* project you may encounter issues with naming; this is because Objects within a *Project* must be _uniquely_ named. If someone
else is doing this course in the same Project they may have created objects with the names stated. +

*If you get an error when trying to create _anything_ as part of this module, add "__(your initials)" to the end of the Name attribute for the object
you are creating*
====

=== *Setting up the example Applications for the module*

First we will setup three simple Applications within the Project. Make sure you are at the Developer Viewpoint. Click *+Add*. Choose *Container images*. Enter the following text into the *Image name from external registry* entry point:

[.console-input]
[source,bash]
----
quay.io/marrober/devex-terminal-4:full-terminal-1.5
----

In *Application* choose *Create Application* and give it the name *networking-app*. Change the *Name* to *application1*. Hit *Create*.

Click *+Add* again. Choose *Import from Git*. Copy the text from below into the *Git Repo URL* textbox:

[.console-input]
[source,bash]
----
https://github.com/rh-uki-openshift-ssa/workshop-app
----

Choose *networking-app* from the *Application* pulldown. Set the *Name* to *application2*. Click *Create*.

Click *+Add* again. Click on *All services*. Click on the *.NET* Builder Image. Click *Create*. Under the *Git Repo URL* pulldown click on *Try sample*. In *Application*
select the *networking-app* option. Give it the *Name* *application3*. Hit *Create*.

image::009-image002.png[The Module Applications,width=550px]

=== *Understanding Networking in OpenShift*

OpenShift has two basic primitives for Networking, *Services* and *Routes*. 

A *Service* provides a port-based ingress point into the _Application_. A *Route* provides a Fully Qualified Domain Name *URI* to ingress traffic into a *Service*.

[sidebar]
.The SDN - Software Defined Network
--
Everything that runs within OpenShift/Kubernetes lives on an internal network, the *SDN*, which covers all of the machines that make up the Cluster.

This Network is completely internalised; it has no visibility of connectivity to the outside world, and is just an overlay network for all the Applications running within the Cluster.

The *Services* provide 'doorbells' into the Application, and are based on individual ports. For instance, if you have an Application that listens on two Ports, you would need two Services for it to be able to receive traffic.

Where it gets interesting is that the *Services* have IP addresses on the SDN and are *singular* regardless of the number of replicas of the Application. So if you have 10 replicas of
your Application running in Pods, each Pod has a unique IP address _in a different range_ to the Service. The Service is a loadbalancer across all the replicas.

*Routes* then provide the abstracted linkage of the outside world into the *Services*, but the nice thing about the Routes is that they can be changed without changing the underlying Services, and can
actually point to *multiple* Services with a % load balancing themselves.
--

For the sake of simplicity, and to highlight a nice feature of the UI, we are going to add links to the *Services* and *Routes* on the lefthand navigation panel. To do this click on *Search*. Click the 
pulldown for *Resources* and search for 'service'. Tick the box next to *Service* and then click on the *Add to navigation*. Then search for 'route', tick the box next
to *Route* and then click on *Add to navigation*.

image::009-image003.png[Selecting add-ons for navigation,width=550px]

=== *Using Shorthand Service names for inter-Application communication*

[TIP]
====
In this exercise we will show how the system provides 'shortcuts' within the Applications for talking to Services in the same Project
====

First, click on the shortcut on the lefthand panel for *Services*.

[TIP]
====
You will see a list of Services with pertinent information about them listed.
====

[NOTE]
====
Here's where it gets interesting. Each of the Services is named after the application it is linked to; when we created the applications giving them a name assigned
that name to all objects tied to that Application, for instance *Deployment*, *Services*, etc. +

The OpenShift SDN provides _resolvable_ DNS entries for all the Services within the Project _by name_. In English it means that, local to the Project, all Applications
can *directly* reference the Services by name only, which is what we will now prove
====

Switch to the *Topology* page and click on the Roundel for *Application1*. The righthand panel should now display the overview information for the *Deployment* 'application1'. In the *Pods* click on the active Pod name. 

[WARNING]
====
We will now be doing some commands within the Terminal for the Pod. The security settings for the Cluster may have a very short inactivity timeout; if you get disconnected simply reconnect by clicking on the prompt and carry on
from where you are in the instructions.
====

Click on *Terminal*. Now enter the following command:

[.console-input]
[source,bash]
----
curl http://application2:8080
----

What you will see is a set of HTML; this is the output of the Application *application2*. Using the 'curl' command we have done a URI pull directly. 

Switch back to the Topology view and select the deployment roundel for Application3. Select the pod for this application and enter a terminal.

Now type:

[.console-input]
[source,bash]
----
env | grep APPLICATION2
----

OpenShift also injects a lot of connectivity information directly into the other Applications for the Service endpoints; note the definitions of protocols, ports and IP addresses.

Filter the output a little with the command:

[.console-input]
[source,bash]
----
env | grep APPLICATION2 | grep TCP=tcp://
----

[TIP]
====
Also note that the *Service* application2 actually has two endpoints defined via the Service. One is for http (8080), one is for https (8443)
====

=== *Using FQDN references for Service communications*

We have shown that you can use the *name* of the *Service* to directly communicate from one Application to another. This is useful because it is a direct reference, meaning
a lack of need for Service discovery and external resolution of the Services. 

OpenShift also provides resolution of the Service using a Fully Qualified Domain Name, rather than the shorthand Service name notation.

Switch to the *Topology* panel. Click on the roundel for *application2*. In the righthand panel which will be showing overview information for the *Deployment* application2, click on the active Pod name.

[WARNING]
====
We will now be doing some commands within the Terminal for the Pod. The security settings for the Cluster may have a very short inactivity timeout; if you get disconnected simply reconnect by clicking on the prompt and carry on
from where you are in the instructions.
====

Click on *Terminal*. Now enter the following command:

[.console-input]
[source,bash]
----
curl http://application1.%PROJECT%.svc.cluster.local:8080
----

[TIP]
====
The format of the FQDN for internal communications with Applications in _any Project you have visibility of_ is (service name).(project name).*svc.cluster.local*
====

You should see the HTML output of the *application1* application via the *application1* service.

[TIP]
====
If you have multiple Projects in OpenShift you can communicate between them using this method. 
====

=== *Introducing Network Policies*

In addition to the *routes* and *Services* OpenShift also provides another important object with regard to Networking - the *Network Policy*.

This object allows you, assuming your Cluster gives you access, to create, delete and maintain objects that control in a fine-grain way the access to the Services in your project.

Using a combination of rules you can specify exactly what can and cannot interact with your Services. 

To start, we will add the Network Policy shortcut to the navigation panel. Click on *Search*. Click on *Resources*. Type *NetworkPolicy*. Click on the box next to (NP) NetworkPolicy. Click on *Add to navigation*
on the righthand side of the panel. 

Now click on *NetworkPolicies*.

[TIP]
====
At this point you may have existing Network Policies in your project. For the sake of this exercise we are going to delete them. +

Network Policies work in a linear fashion; if you have, for example, a policy defined that says 'everything can access all services in your namespace', this will overwrite any we add
so to demonstrate the principles we are going to start with an empty set. +

For each of the Network Policies that exist, if any, click on the 'kebab' (the three vertically arranged dots at the far right of the Policy listing) and select
*Delete Network Policy*. When prompted for confirmation click 'Delete'.
====

[WARNING]
====
The Cluster you are using may be setup with stricter security disallowing you from deleting Network Policies. If so simply read the rest of the Module for information before following the tidy-up steps.
====

We have three Applications running in our project. What we are going to do is restrict access to application1; we are going to produce a Network Policy that allows traffic in
only from application2.

To do this click on *Create NetworkPolicy*.

[TIP]
====
The creation of Network Policies is provided via two mechanisms; you can use the *Form view* to build the rules in a wizard fashion, or enter YAML as with all Objects. For the
sake of this exercise we will use the *YAML View*.
====

Switch the view to *YAML View* by clicking on the appropriate radio button in the *Configure via:* tab.

Clear the contents of the YAML editor and replace with the following:

[.console-input]
[source,bash]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allowapplication2
spec:
  podSelector:
    matchLabels:
      app: application1
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: application2
  policyTypes:
    - Ingress
----

[TIP]
====
In English what this Policy is stating is: +

For all Pods with a label of *"app: application1"* only allow ingress from Pods with a label of *"app: application2"*
====

We will now shows this in action. Click on *Topology*. Click on the *application2* roundel. In the righthand panel click on the active Pod (the name). When the Pod overview
page appears click on *Terminal*. In the Terminal type:

[.console-input]
[source,bash]
----
curl http://application1:8080
----

You should see the response webpage source displayed. Now click on *Topology* again. Click on the *application3* roundel. In the righthand panel click on the active Pod (the name). When the Pod
overview appears click on *Terminal*. Again, type the following in the Terminal:

[.console-input]
[source,bash]
----
curl http://application1:8080
----

This time the terminal will hang; the Pod cannot connect to that service endpoint.

This was a very simple example; the capabilities of the Network Policies are very powerful. One of the nice features of the OpenShift Ux is the YAML view on creation of
a Network Policy provides some useful archetypes; if you switch the righthand panel to 'Samples' you can download example YAML or even use 'Try it' which will provide the YAML
directly into the Editor. 

image::009-image004.png[Example Network Policies,width=400px]

=== Cleaning up

[TIP]
====
When you create Applications in OpenShift they will remain resident until you remove them
====

To finish the Module head to the *Topology page*, click on each of the *Application Groups* (i.e. (A) networking-app) and in the *Actions* menu on the righthand panel for the Application choose *Delete Application*.
The system will prompt you to enter the name of the Application Group; enter this name and press return/hit *Delete*.

[TIP]
====
Deleting the Application Group removes all of the Objects relating to the application
====













