= Adding Persistent Storage to Applications
:!sectids:

image::008-image001.png[Setting up Storage]

== *Adding Persistent Storage to Applications*

====
*What will you learn?*

The Module introduces the concepts of Persistent Storage, which gives Applications storage that exists and persists beyond the lifespan of the Pod running in OpenShift

This will demonstrate the concept, usecases and constructs used to create, manipulate and retain storage for Applications

After completing this Module you will understand the concepts of Persistent Storage with regard to Applications in OpenShift
====

=== *Pre-Requisites*

In order to undertake the Module you need to be logged onto the OpenShift cluster and have access to a Project. If you do not have rights to create a project, ask the Cluster administrators to provide one for you.

*Ensure the two textboxes at the top of this HTML Guide to the Module contain the Cluster address and the Project you will be using.* 

[NOTE]
====
A *Project* is an OpenShift object that contains all of your Applications and other components in a compartmented way. +
As the owner/user of a Project you have 'Admin' rights to it, meaning you can create any of the OpenShift objects that you have the rights to create within it.  
====

[WARNING]
====
*Please Read* - if you are using a *shared* project you may encounter issues with naming; this is because Objects within a *Project* must be _uniquely_ named. If someone
else is doing this course in the same Project they may have created objects with the names stated. +

*If you get an error when trying to create _anything_ as part of this module, add "__(your initials)" to the end of the Name attribute for the object
you are creating*
====

=== *Setting up an example Application for the module*

First we will setup an example Application to show the use of Persistent Volumes and Storage within OpenShift

Make sure you are at the Developer Viewpoint and viewing the *Topology* page. Click *+Add* then *Import from Git*. In the Git repo textbox paste the value from below:

[.console-input]
[source,bash]
----
https://github.com/rh-uki-openshift-ssa/envchecker
----

Set the *Application* (which is the Application Grouping) to *storage-app* by selecting *Create Application* from the pulldown and entering storage-app into the textbox. Change the name to *storageexample*. Set the *Resource* type to generate to *Deployment*. Hit *Create* to start the build.

=== *Creating Storage for Consumption*

We will now create some storage for the Application to use. In order to do this we need to understand some of the concepts behind how Storage works within Kubernetes/OpenShift.

[sidebar]
.Ephemeral vs Persisted Storage
--
Kubernetes and OpenShift are designed to be Cloud Native in the way they handle Applications. Put simply, Applications are now deemed to be *Cattle* instead of *Pets*.

This was a fantastic analogy originally from Microsoft that defines something to be a pet if you nurture it's environment and do all you can to keep it alive. Cattle defines a workload that can be lost 
and simply restarted. This requires a degree of engineering within an Application to make it stateless.

Basically an Application should 'outsource' all its state that it needs to retain between incarnations. Effectively they should be *sausage machines* and all configuration and persistence
should be done external to the Applications.

This lends itself perfectly to the concept of Kubernetes/OpenShift hosted Applications, as the Pods are designed to be ephemeral and self-healing; if you lose a Pod OpenShift
will automatically recreate it somewhere else to make sure the object state required, stored in the brain (*etcd*), matches the physical state of the system.

Also Containers are built from immutable Images. If you lose a Container when it is restored it only contains the file-systems as they are defined in the Image. 

To support some post-execution file persistence Kubernetes and OpenShift support the concept of *Persistent Volumes*, external storage that can be expressed as an interactable 
file system in the Containers readable/writeable storage space.

This storage is hosted *external* to the Container, meaning it persists after the Container is shutdown, and can be restored to a copy of the Container 
--

We are now going to create a pot of storage for our Application to use. 

[TIP]
====
We are also going to take advantage of a useful feature of the Developer interface; you can *search* for objects of a certain type and then add them to the lefthand havigation panel. For ease of use we are going to do this for the *PVC* *Persistent Volume Claims*.
====

Click on *Search* on the lefthand panel. Click on the *Resources* pulldown. Enter *PVC* in the search box. Click on the *PVC* option. The panel will show as below:

image::008-image002.png[PVC Panel,width=550px]

Click on the *+ Add to navigation* link. The shortcut to that object type will be displayed in the lower left part of the navigation. You can add any of the resources that can be rendered here as useful shortcuts
while developing. To remove them simply place the mouse over the item and click on the *-* symbol on the right of the entry.

Now click on the *Create PersistentVolumeClaim* button. 

[TIP]
====
The distinction between PersistentVolume and PersistentVolumeClaim is worth understanding at this point. The PersistentVolume is a physical volume maintained by the OpenShift system. It is Cluster-wide, not
bound to a named Project, but can be reserved and locked to a Project using a *PersistentVolumeClaim*. 

If a *PersistentVolume* is *bound* it is locked to a *PVC*. The PersistentVolume itself has a number of determinators

*Reclaim Policy* informs the Cluster what to do with the *PV* once it has no *PVCs* claiming it. This policy can be 'Retain', which keeps the data on the storage even when it is not bound to a PVC, or 'Delete', which automatically hard wipes the data when there are no bindings to a PVC.

*Access modes* is an important one. This can be *ReadOnly*, *ReadWriteOnce* or *ReadWriteMany*. If a PV is *ReadOnly* it can only be read by the Container it is attached to (it is mounted read only). If a PV is *ReadWriteOnce* it is read/write for the Container it is attached to *but* it can only be attached to *one* instance of the container per worker node. If it is 
*ReadWriteMany* it can be attached and consumed by *multiple* Containers _at the same time_. This provides a singular source of storage that is shared, in real time, by multiple replicas of the same Application.
====

In the dialog for creating a PVC leave the storage class as it is (if you hit the pulldown you will see the types of storage that can be used; the *StorageClass* is an object installed by
the administrators for cookie-cutting various storage types. 

image::008-image003.png[PVC dialog,width=500px]

For the *PersistentVolumeClaim name* enter:

[.console-input]
[source,bash]
----
examplestorage
----

Leave the *Access mode* on *Single user (RWO)*. Set the size of the claim to *1 GB*. Leave the *Volume mode* as *filesystem*.

Hit *Create*. The system should respond as shown below:

image::008-image004.png[Active PVC,width=500px]

=== *Attaching Storage to an Application*

At this point we have a *PVC* owned by the projects. To confirm this click on the *PersistentVolumeClaims* link on the lefthand navigation panel. There should be a single PVC
called *examplestorage* with a status of *Bound* and a capacity of *1 GB*.

[CAUTION]
====
With some storage systems the PVC may remain as 'Pending' until it is used. This is not an issue; attaching the PVC to the Application, as you
will now do, will instantiate and bind the storage
====

This piece of storage is now bound to the _project_ but not bound to the _application_. To do that we need to add the Storage to the *Deployment*, telling OpenShift how to 
attach the file system into the Application itself.

Click on *Topology*. Click on the Roundel for *storageexample*. Select the *Actions* pulldown next to the *D* storageexample and click *Add storage*.

In the *Add Storage* dialog leave *Use existing claim* selected and click on *Select claim*. Choose *examplestorage*.

[TIP]
====
You can shortcut the process by creating the PVC in this dialog; we pre-created it to show the separate PVC components.
====

In the *Mount path* copy the text from below:

[.console-input]
[source,bash]
----
/ocpintro/test
----

When you hit *Save* the deployment will be displayed and the Application will be re-deployed. Watch the process, and then when the new version has completed click on the *Pods* tab on the deployment panel.

=== *Consuming storage in an Application Pod* 

In the list of *Pods* there should be one running instance of the *storageexample*. Click on the name of this Pod.

In the *Pod details* panel click on the *Terminal* tab.

[WARNING]
====
The security settings for the Cluster will disconnect the Terminal window after a very short inactivity period. If it disconnects during this part of the Module simply click on *Reconnect*.
====

In the Terminal apply the following commands:

[.console-input]
[source,bash]
----
df -h
cd /ocpintro/test
touch test.txt
ls -al
----

The resulting Terminal should look like this:

image::008-image005.png[Terminal Output,width=550px]

[TIP]
====
By doing *df -h* you display the current file devices attached to the container; note the addition of one for */ocpintro/test*.

We have then created a file (albeit a 0 length file) using *touch* and done a directory listing to see it there.
====

The file is now available within the Container.

=== *Showing the ReadWriteOnce behaviour*

Switch back to the *Topology* view. Click on the Roundel to display the deployment storageexample. Click on the *Details* tab. Scale the Application up to two replicas by clicking on the up arrow next to the Roundel displayed
in the deployment panel.

If you wait a little while then click on *Observe* on the lefthand panel  (not the observe section of the deployment details). Switch to the *Events* tab. You should see an event that indicates a 'Multi-Attach' error. *This is as expected*. We set the behaviour of the *PV* to ReadWriteOnce, it can only be mounted to one active Container.

Switch back to the *Topology* panel and following the instructions above set the number of replicas for the deployment to 1 rather than 2.

=== *Demonstrating Storage Persistence*

Now we are going to completely remove the running instances. Using the down arrow next to the Roundel in the deployment panel reduce the number of replicas of the Application to *0*.f

image::008-image006.png[Scaled down to 0,width=550px]

At this point there are no Applications active. Using the up arrow next to the Roundel in the deployment panel increase the replicas back to *1*.

[TIP]
====
The Application is recreated from the original immutable Image. *However* the deployment specifies a connection to the external storage via the PVC.
====

Click on the *Resources* tab in the deployment panel and click on the single active *Pod* name. In the *Pod details* panel click on the *Terminal* tab. Execute the following commands:

[.console-input]
[source,bash]
----
cd /ocpintro/test
ls -al
----

[TIP]
====
The file created internally in the last Pod will still be there; the Pod has been recreated and reattached to the same physical storage point.
====

=== Cleaning up

[TIP]
====
When you create Applications in OpenShift they will remain resident until you remove them
====

To finish the Module head to the *Topology page*, click on each of the *Application Groups* (i.e. (A) storage-app) and in the *Actions* menu on the righthand panel for the Application choose *Delete Application*.
The system will prompt you to enter the name of the Application Group; enter this name and press return/hit *Delete*.

Also go to the *PersistentVolumeClaims* panel (via your shortcut on the lefthand navigation panel). Remove the PVC if it remains (click on the three dot/kebab at the far right and choose *Delete PersistentVolumeClaim*). 

[TIP]
====
Deleting the Application Group removes all of the Objects relating to the application
====










